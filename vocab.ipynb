{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# def put_bos_eos(tokens_lists):\n",
    "#     return [[(\"xxbos\", \"SPCL_BRIDGE\"), *tokens, (\"xxeos\", \"SPCL_BRIDGE\")] for tokens in tokens_lists]\n",
    "\n",
    "# def deal_with_caps(tokens_lists):\n",
    "#     rlist = []\n",
    "#     for tokens in tokens_lists:\n",
    "#         nlist = []\n",
    "#         for token in tokens:\n",
    "#             nlist.append((token[0].lower(), token[1]))\n",
    "#             if(token[0].isupper()):\n",
    "#                 nlist.append((\"xxcap\", \"SPCL_APPEND\"))\n",
    "#             elif(token[0][0].isupper()):\n",
    "#                 nlist.append((\"xxmaj\", \"SPCL_APPEND\"))\n",
    "#         rlist.append(nlist)\n",
    "#     return rlist\n",
    "\n",
    "# class Vocab():\n",
    "#     def __init__(self, text_list, path_list=None, nlp=None, apply_defaults=True, callbacks=[]):\n",
    "#         assert type(text_list) == list\n",
    "#         assert type(callbacks) == list\n",
    "#         self.nlp, self.apply_defaults = nlp, apply_defaults\n",
    "#         self.callbacks = [put_bos_eos, deal_with_caps] if(self.apply_defaults) else []\n",
    "#         self.callbacks.extend(callbacks)\n",
    "#         if(self.nlp is None): self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "#         self.tokens_lists = self.get_list_of_list_of_tokens(text_list)\n",
    "#         self.tokens_lists = self.apply_callbacks(self.tokens_lists)\n",
    "#         self.itos, self.stoi = self.create_vocab(self.tokens_lists)\n",
    "#         self.batches = [[token for tokens in self.tokens_lists for token in tokens]]\n",
    "\n",
    "#         self.tokens_base = Path('.')\n",
    "#         self.dump_tokens(path_list, )\n",
    "#         self.leaf_types = {\n",
    "#             \"ROOT_LEAF\": 0,\n",
    "#             \"ONE_CHILD_LEAF\": 1,\n",
    "#             \"TWO_CHILD_LEAF\": 2,\n",
    "#             \"BRIDGE_LEAF\": 3,\n",
    "#             \"INVARIANT\": 4,\n",
    "#         }\n",
    "        \n",
    "#         # NOTE: subject to change...INTJ is not tested\n",
    "#         self.word_to_leaf_type = {\n",
    "#             \"PUNCT\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "#             \"SYM\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"ADJ\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"ADP\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"CCONJ\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "#             \"NUM\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"DET\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"ADV\": self.leaf_types[\"TWO_CHILD_LEAF\"],\n",
    "#             \"X\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"VERB\": self.leaf_types[\"TWO_CHILD_LEAF\"],\n",
    "#             \"NOUN\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"PROPN\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"PART\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"INTJ\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "#             \"PRON\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "#             \"SPCL_APPEND\": self.leaf_types[\"INVARIANT\"],\n",
    "#             \"SPCL_BRIDGE\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "#             \"ROOT\": self.leaf_types[\"ROOT_LEAF\"]\n",
    "#         }\n",
    "\n",
    "#     def __str__(self):\n",
    "#         return f\"{self.tokens_lists}\\n\\n{self.itos}\\n\\n{self.stoi}\\n\\n\"\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         return f\"{self.tokens_lists}\\n\\n{self.itos}\\n\\n{self.stoi}\\n\\n\"\n",
    "    \n",
    "#     def unsimplified_tagged_list(self, sentence):\n",
    "#         tokens = word_tokenize(sentence)\n",
    "#         return pos_tag(tokens)\n",
    "\n",
    "#     def simplified_tagged_list(self, sentence):\n",
    "#         doc = self.nlp(sentence)\n",
    "#         tags = [(token.text, token.pos_) for token in doc]\n",
    "#         return tags\n",
    "    \n",
    "#     def get_batched_token_sequences(self):\n",
    "# #         self.batches =  [[token for tokens in self.tokens_lists for token in tokens]]\n",
    "#         return self.batches\n",
    "    \n",
    "#     def get_list_of_list_of_tokens(self, text_list):\n",
    "#         tokens_lists = []\n",
    "#         for text in text_list:\n",
    "#             tokens_lists.append(self.simplified_tagged_list(text))\n",
    "#         return tokens_lists\n",
    "    \n",
    "#     def create_vocab(self, tokens_lists):\n",
    "#         all_tokens = [item for sublist in tokens_lists for item in sublist]\n",
    "#         all_tokens = set(all_tokens)\n",
    "#         itos, stoi = [], {}\n",
    "#         i = 0\n",
    "#         for token, wtype in all_tokens:\n",
    "#             stoi[token] = i\n",
    "#             itos.append((token, wtype))\n",
    "#             i += 1\n",
    "#         return itos, stoi \n",
    "    \n",
    "#     def apply_callbacks(self, tokens_lists):\n",
    "#         for callback in self.callbacks:\n",
    "#             tokens_lists = callback(tokens_lists)\n",
    "#         return tokens_lists\n",
    "\n",
    "#     def get_ltype(self, token):\n",
    "#         return self.word_to_leaf_type[token]\n",
    "\n",
    "#     def get_ltypes_dict(self):\n",
    "#         return self.leaf_types\n",
    "    \n",
    "#     def get_seq_at_index(self, index):\n",
    "#         return self.batches[index]\n",
    "    \n",
    "#     def get_indices_from_tokens(self, tokens):\n",
    "#         return [self.stoi[token[0]] for token in tokens]\n",
    "\n",
    "#     def get_word_tensors(self, tokens):\n",
    "#         indices = self.get_indices_from_tokens(tokens)\n",
    "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         return torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator():\n",
    "    def __init__(self, path_list, text_list=None, encoding=\"utf8\"):\n",
    "        assert type(path_list) == list or type(text_list) == list\n",
    "        if(path_list is None):\n",
    "            self.path_list = self.create_data_paths(text_list)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.path_list, self.encoding = path_list, encoding\n",
    "    \n",
    "    def create_data_paths(self, base_path, text_list):\n",
    "        i = 0\n",
    "        paths = []\n",
    "        for text in text_list:\n",
    "            dest_path = base_path / f\"{i}.txt\"\n",
    "            paths.append(dest_path)\n",
    "            with open(dest_path, \"w\") as f:\n",
    "                f.write(text)\n",
    "        return paths\n",
    "    \n",
    "    def get_next_text(self):\n",
    "        for p in self.path_list:\n",
    "            with open(p, 'r',encoding=self.encoding) as f:\n",
    "                s = f.read()\n",
    "                yield p, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, tokenizer, max_len=20000):\n",
    "        self.tokenizer, self.max_len = tokenizer, max_len\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.itos, self.stoi, self.stoc = [], {}, {}\n",
    "        self.special_tokens = [\"xxbos\", \"xxeos\", \"xxmaj\", \"xxcap\", \"xxunk\"]\n",
    "        \n",
    "    def trim_word_index_mappings(self):\n",
    "        sorted_stoc = {k: v for k, v in reversed(sorted(self.stoc.items(), key=lambda item: item[1]))}\n",
    "        for k in self.special_tokens:\n",
    "            if k in sorted_stoc: del sorted_stoc[k]\n",
    "        sorted_list = list(sorted_stoc.items())\n",
    "        \n",
    "        new_stoi, new_itos, new_stoc = {}, [], {}\n",
    "        for spcl_tok in self.special_tokens:\n",
    "            cur_ind = len(new_itos)\n",
    "            new_stoi[spcl_tok] = cur_ind\n",
    "            new_itos.append(spcl_tok)\n",
    "            \n",
    "        for i in range(min(self.max_len, len(sorted_list))):\n",
    "            k, v = sorted_list[i]\n",
    "            cur_ind = len(new_itos)\n",
    "            new_stoc[k], new_stoi[k] = v, cur_ind\n",
    "            new_itos.append(k)\n",
    "        self.itos, self.stoi, self.stoc = new_itos, new_stoi, new_stoc\n",
    "    \n",
    "    def update_token_list(self, token_list):\n",
    "        new_list = []\n",
    "        for token in token_list:\n",
    "            tok, ltype = token\n",
    "            if tok not in self.stoi: new_list.append((\"xxunk\", \"SPCL_APPEND\"))\n",
    "            else: new_list.append((tok, ltype))\n",
    "        return new_list\n",
    "    \n",
    "    def push_tok_to_word_index_mappings(self, tok):\n",
    "        if tok not in self.stoi:\n",
    "            ind = len(self.itos)\n",
    "            self.stoi[tok], self.stoc[tok] = ind, 1\n",
    "            self.itos.append(tok)\n",
    "        else:\n",
    "            self.stoc[tok] += 1\n",
    "        \n",
    "    def prepare(self):\n",
    "        for _,token_list in self.tokenizer.get_next_token_list():\n",
    "            for tok,_ in token_list:\n",
    "                self.push_tok_to_word_index_mappings(tok)\n",
    "        self.trim_word_index_mappings()\n",
    "        \n",
    "    def get_trimmed_tokens(self, token_list):\n",
    "        return self.update_token_list(token_list)    \n",
    "\n",
    "    def get_indices_from_tokens(self, tokens):\n",
    "        return [self.stoi[token[0]] for token in tokens]\n",
    "\n",
    "    def get_word_tensors(self, tokens):\n",
    "        indices = self.get_indices_from_tokens(tokens)\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return torch.tensor(indices, dtype=torch.long, device=device).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, path_list, dest_path=Path('./data'), apply_defaults=True, max_len=20000, callbacks=[]):\n",
    "        self.text_generator = TextGenerator(path_list)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.tokens_paths = []\n",
    "        self.callbacks = [self.put_bos_eos, self.deal_with_caps] if(apply_defaults) else []\n",
    "        self.callbacks.extend(callbacks)\n",
    "        self.prepare_initial_tokens(path_list, dest_path)\n",
    "        self.vocab = Vocab(self, max_len)\n",
    "        self.vocab.prepare()\n",
    "        self.update_tokens_lists()\n",
    "        \n",
    "        self.leaf_types = {\n",
    "            \"ROOT_LEAF\": 0,\n",
    "            \"ONE_CHILD_LEAF\": 1,\n",
    "            \"TWO_CHILD_LEAF\": 2,\n",
    "            \"BRIDGE_LEAF\": 3,\n",
    "            \"INVARIANT\": 4,\n",
    "        }       \n",
    "        self.word_to_leaf_type = {\n",
    "            \"PUNCT\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "            \"SYM\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"ADJ\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"ADP\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"CCONJ\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "            \"NUM\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"DET\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"ADV\": self.leaf_types[\"TWO_CHILD_LEAF\"],\n",
    "            \"X\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"VERB\": self.leaf_types[\"TWO_CHILD_LEAF\"],\n",
    "            \"NOUN\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"PROPN\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"PART\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"INTJ\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "            \"PRON\": self.leaf_types[\"ONE_CHILD_LEAF\"],\n",
    "            \"SPCL_APPEND\": self.leaf_types[\"INVARIANT\"],\n",
    "            \"SPCL_BRIDGE\": self.leaf_types[\"BRIDGE_LEAF\"],\n",
    "            \"ROOT\": self.leaf_types[\"ROOT_LEAF\"]\n",
    "        }\n",
    "        \n",
    "    def put_bos_eos(self, tokens_list):\n",
    "        return [(\"xxbos\", \"SPCL_BRIDGE\"), *tokens_list, (\"xxeos\", \"SPCL_BRIDGE\")]\n",
    "\n",
    "    def deal_with_caps(self, token_list):\n",
    "        nlist = []\n",
    "        for token in token_list:\n",
    "            nlist.append((token[0].lower(), token[1]))\n",
    "            if(token[0].isupper()):\n",
    "                nlist.append((\"xxcap\", \"SPCL_APPEND\"))\n",
    "            elif(token[0][0].isupper()):\n",
    "                nlist.append((\"xxmaj\", \"SPCL_APPEND\"))\n",
    "        return nlist\n",
    "\n",
    "    def apply_callbacks(self, token_list):\n",
    "        for callback in self.callbacks:\n",
    "            token_list = callback(token_list)\n",
    "        return token_list\n",
    "\n",
    "    def prepare_initial_tokens(self, src_paths, dest_root):\n",
    "        for path, content in self.text_generator.get_next_text():\n",
    "            fname = path.stem\n",
    "            dest_path = dest_root / f\"{fname}.pkl\"\n",
    "            self.tokens_paths.append(dest_path)\n",
    "            token_list = self.simplified_tagged_list(content)\n",
    "            token_list = self.apply_callbacks(token_list)\n",
    "            with open(dest_path, 'wb') as f:\n",
    "                pickle.dump(token_list, f)\n",
    "    \n",
    "    def get_next_token_list(self):\n",
    "        for p in self.tokens_paths:\n",
    "            with open(p, \"rb\") as f:\n",
    "                tk_list = pickle.load(f)\n",
    "            yield p, tk_list\n",
    "\n",
    "    def update_tokens_lists(self):\n",
    "        for p in self.tokens_paths:\n",
    "            with open(p, \"rb\") as f:\n",
    "                tk_list = pickle.load(f)\n",
    "                new_tk_list = self.vocab.get_trimmed_tokens(tk_list)\n",
    "            with open(p, \"wb\") as f:\n",
    "                pickle.dump(new_tk_list, f)\n",
    "    \n",
    "    def simplified_tagged_list(self, sentence):\n",
    "        doc = self.nlp(sentence)\n",
    "        tags = [(token.text, token.pos_) for token in doc]\n",
    "        return tags\n",
    "    \n",
    "    def get_ltype(self, token):\n",
    "        return self.word_to_leaf_type[token]\n",
    "\n",
    "    def get_ltypes_dict(self):\n",
    "        return self.leaf_types\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer([Path('./test.txt')], max_len=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Path('./test.txt')\n",
    "p.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'Hariom is playing',\n",
    "    'I got milk from the store',\n",
    "    'from the store, I got milk',\n",
    "    'hey hey hey',\n",
    "    'this sentence can be structured to a tree in a systematic manner',\n",
    "    'what is he doing',\n",
    "    'sentence illegal is',\n",
    "    'sentence is legal',\n",
    "    'This sentence is going to be long because we need to check what is going on',\n",
    "    'The poop is kept on the beautiful table',\n",
    "    'bread and butter is good for health',\n",
    "    'wine is great with bread and butter is good for health'\n",
    "]\n",
    "\n",
    "v = Vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40],\n",
       "        [44],\n",
       "        [47],\n",
       "        [21],\n",
       "        [35],\n",
       "        [ 2]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.tensor_from_tokens(v.tokens_lists[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(vocab, sentence):\n",
    "    return [vocab.stoi[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "#     indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=\"cpu\").view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[44],\n",
       "        [21],\n",
       "        [35]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorFromSentence(v, 'hariom is playing') # gives a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxbos', 'SPCL_BRIDGE'),\n",
       " ('hariom', 'PROPN'),\n",
       " ('xxmaj', 'SPCL_APPEND'),\n",
       " ('is', 'VERB'),\n",
       " ('playing', 'VERB'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('i', 'PRON'),\n",
       " ('xxcap', 'SPCL_APPEND'),\n",
       " ('got', 'VERB'),\n",
       " ('milk', 'NOUN'),\n",
       " ('from', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('store', 'NOUN'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('from', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('store', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('i', 'PRON'),\n",
       " ('xxcap', 'SPCL_APPEND'),\n",
       " ('got', 'VERB'),\n",
       " ('milk', 'NOUN'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('hey', 'INTJ'),\n",
       " ('hey', 'INTJ'),\n",
       " ('hey', 'INTJ'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('this', 'DET'),\n",
       " ('sentence', 'NOUN'),\n",
       " ('can', 'VERB'),\n",
       " ('be', 'VERB'),\n",
       " ('structured', 'VERB'),\n",
       " ('to', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('tree', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('a', 'DET'),\n",
       " ('systematic', 'ADJ'),\n",
       " ('manner', 'NOUN'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('what', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('he', 'PRON'),\n",
       " ('doing', 'VERB'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('sentence', 'NOUN'),\n",
       " ('illegal', 'ADJ'),\n",
       " ('is', 'VERB'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('sentence', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('legal', 'ADJ'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('this', 'DET'),\n",
       " ('xxmaj', 'SPCL_APPEND'),\n",
       " ('sentence', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('going', 'VERB'),\n",
       " ('to', 'PART'),\n",
       " ('be', 'VERB'),\n",
       " ('long', 'ADJ'),\n",
       " ('because', 'ADP'),\n",
       " ('we', 'PRON'),\n",
       " ('need', 'VERB'),\n",
       " ('to', 'PART'),\n",
       " ('check', 'VERB'),\n",
       " ('what', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('going', 'VERB'),\n",
       " ('on', 'PART'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('the', 'DET'),\n",
       " ('xxmaj', 'SPCL_APPEND'),\n",
       " ('poop', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('kept', 'VERB'),\n",
       " ('on', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('beautiful', 'ADJ'),\n",
       " ('table', 'NOUN'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('bread', 'NOUN'),\n",
       " ('and', 'CCONJ'),\n",
       " ('butter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('good', 'ADJ'),\n",
       " ('for', 'ADP'),\n",
       " ('health', 'NOUN'),\n",
       " ('xxeos', 'SPCL_BRIDGE'),\n",
       " ('xxbos', 'SPCL_BRIDGE'),\n",
       " ('wine', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('great', 'ADJ'),\n",
       " ('with', 'ADP'),\n",
       " ('bread', 'NOUN'),\n",
       " ('and', 'CCONJ'),\n",
       " ('butter', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('good', 'ADJ'),\n",
       " ('for', 'ADP'),\n",
       " ('health', 'NOUN'),\n",
       " ('xxeos', 'SPCL_BRIDGE')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.get_seq_at_index(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
